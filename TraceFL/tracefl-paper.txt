TraceFL: Interpretability-Driven Debugging in
 Federated Learning via Neuron Provenance
 Waris Gill
 Computer Science Department
 Virginia Tech
 Blacksburg, USA
 waris@vt.edu
 arXiv:2312.13632v4  [cs.LG]  17 Jan 2025
 Ali Anwar
 Computer Science and Engineering Department
 University of Minnesota
 Minneapolis, USA
 aanwar@umn.edu
 Abstract—In Federated Learning, clients train models on local
 data and send updates to a central server, which aggregates them
 into a global model using a fusion algorithm. This collaborative
 yet privacy-preserving training comes at a cost. FL developers
 face significant challenges in attributing global model predictions
 to specific clients. Localizing responsible clients is a crucial step
 towards (a) excluding clients primarily responsible for incorrect
 predictions and (b) encouraging clients who contributed high
quality models to continue participating in the future. Existing
 ML debugging approaches are inherently inapplicable as they
 are designed for single-model, centralized training.
 We introduce TraceFL, a fine-grained neuron provenance
 capturing mechanism that identifies clients responsible for a
 global model’s prediction by tracking the flow of information
 from individual clients to the global model. Since inference on
 different inputs activates a different set of neurons of the global
 model, TraceFL dynamically quantifies the significance of the
 global model’s neurons in a given prediction, identifying the most
 crucial neurons in the global model. It then maps them to the
 corresponding neurons in every participating client to determine
 each client’s contribution, ultimately localizing the responsible
 client. We evaluate TraceFL on six datasets, including two
 real-world medical imaging datasets and four neural networks,
 including advanced models such as GPT. TraceFL achieves 99%
 accuracy in localizing the responsible client in FL tasks spanning
 both image and text classification tasks. At a time when state
of-the-art ML debugging approaches are mostly domain-specific
 (e.g., image classification only), TraceFL is the first technique to
 enable highly accurate automated reasoning across a wide range
 of FL applications.
 Muhammad Ali Gulzar
 Computer Science Department
 Virginia Tech
 Blacksburg, USA
 gulzar@cs.vt.edu
 output? This question is akin to debugging software, where
 understanding the impact of each input and the line of code
 on the software’s output is crucial. Addressing this debugging
 question is vital for the effective deployment, maintenance,
 and accountability of FL applications. For example, FL de
velopers face challenges in identifying and rewarding clients
 responsible for successful classifications. This recognition is
 crucial to encourage their continued participation in future
 incentivized FL rounds [6]. There is mature evidence that such
 practice significantly improves the FL model’s quality [7].
 Similar debugging is key in localizing faulty clients that may
 transfer an inaccurate model for aggregation, which can result
 in a dangerously low-quality global model [8]–[12].
 Index Terms—Interpretability, Explainability, Debugging, Ma
chine Learning, Federated Learning, Transformer
 I. INTRODUCTION
 Federated Learning (FL) offers distributed training that
 enables multiple clients to collaboratively train a global model
 without sharing raw data [1]–[5]. In a typical FL setup,
 individual clients, such as healthcare institutions, train models
 on their local data. These local models are then aggregated
 on a central server to form a comprehensive global model,
 all without transferring sensitive client data. The resulting
 global model, a fusion of all clients’ models, is then used
 in production to make predictions on unseen data.
 The complexity of FL systems, however, introduces unique
 debugging challenges. When a global model makes a pre
diction, whether correct or incorrect, a key question arises:
 which client(s) is primarily responsible for a global model’s
 Problem. In federated learning, the client(s) most responsible
 for a global model’s prediction are the ones trained on
 data that contains the predicted labels. This is analogous
 to finding influential training samples in classical machine
 learning [13]. However, the two domains, single model-based
 centralized ML and FL, are fundamentally different. Existing
 influence-based debugging approaches in ML [14]–[17] and
 regular software [18]–[20] require transparent access to data
 including all data manipulation operations applied on the input
 data. When applied to FL, these approaches will require end
to-end monitoring of clients’ training (i.e., require access
 to clients’ data), which is prohibited in FL. More broadly,
 MLinfluence and interpretability-based debugging approaches
 target a single model in which the debugging is restricted to
 identifying the training data. In contrast, debugging in FL
 entails isolating a client’s model among many. This paper
 addresses the following debugging problem in FL: Given the
 global model inference on an input in FL, how can we identify
 the client(s) most responsible for the inference?
 Challenges. Determining a client’s influence on the global
 model is challenging. Clients are randomly sampled in each
 round, each possessing unique data and contributing differently
 to the global model. Thus, the influence of a client on the
 global model is dynamic, non-uniform, and changes across
 rounds, making it difficult to link the global model’s behavior
 to a specific client. The FL protocol restricts access to client
side training, turning FL configuration into a nearly black
box setting. Additionally, clients’ models are collections of
 neuron weights that are individually uninterpretable. Static
 1
Accepted at 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)
 analysis of models’ weights to measure clients’ influence is
 ineffective because clients’ models are intrinsically different
 in terms of weights. Furthermore, neural networks today
 comprise millions of neurons (e.g., GPT-3 has 175 billion
 parameters [21]). Considering all neurons equally, in such
 cases, would lead to imprecise and incorrect debugging.
 FL is increasingly used for domains other than vision using
 various neural networks, such as transformer and convolutional
 neural networks (CNNs). Designing a generic FL debugging
 approach is a major challenge. For instance, transformers
 contain a self-attention mechanism that allows the model to
 focus on different parts of the input sequence. This mechanism
 is usually not seen in CNNs; instead, it uses a convolutional
 layer to detect the special patterns in the input data. Addi
tionally, these architectures use different activation functions
 such as Rectified Linear Unit (ReLU) [22] and Gaussian
 Error Linear Unit (GELU) [23], introducing another source
 of complications.
 Our Contribution. We present the concepts of neuron prove
nance, a fine-grained lineage-capturing mechanism that for
mulates the flow of information in the fusion algorithm from
 multiple clients’ models into a global FL model, ultimately
 influencing the predictions of the global model. Using neuron
 provenance, we determine the precise magnitude of contri
butions of participating clients towards the global model’s
 prediction. We materialize the idea of neuron provenance in
 TraceFL, which runs at the aggregator (i.e., central server) and
 requires no instrumentation on the client side.
 TraceFL is designed with the following insights. Since a
 global model consists of millions of neurons, we observe
 that a dynamic subset of neurons activates in response to
 a given input, and not all neurons contribute equally to a
 prediction [24], [25]. Using this insight, TraceFL quantifies the
 contribution of these neurons in the global model’s prediction
 by computing the gradient of the neurons w.r.t. to the predic
tion. Such neuron-level gradients reveal the neuron’s output
 impact on the global model’s prediction and thus reduce the
 scope of important neurons.
 TraceFL then maps the global model’s important neurons
 to the corresponding neurons in each client’s model and
 computes the contribution of each client’s neuron to the
 corresponding global model’s neuron. At this stage, TraceFL
 computes the end-to-end neuron provenance of the global
 model prediction with the magnitude of the contribution of
 each client’s neurons. Finally, TraceFL aggregates the contri
butions of each client. The client with the highest contribution
 is deemed the most responsible for the given prediction.
 Evaluations. We demonstrate TraceFL’s effectiveness, gener
alizability, and robustness by evaluating its client localization
 accuracy on both image and language models under various
 commercial data distributions and differential privacy methods.
 We evaluate TraceFL on four state-of-the-art neural networks:
 ResNet [26], DenseNet [27], BERT [28], and GPT [29] and
 using six datasets including two real-world medical imaging
 datasets [30]–[32]. TraceFL achieves an average accuracy of
 99% in localizing the responsible client across 30 unique FL
 settings, spanning both correct and misprediction scenarios.
 For fault localization in FL, TraceFL achieves an average
 accuracy of 99%, compared to 32% by the existing tech
nique [33], demonstrating TraceFL’s effectiveness in real
world FL deployments. Additionally, we test TraceFL’s robust
ness against varying data distributions and differential privacy
 settings and find that TraceFL remains robust and effective.
 We also vary the number of clients, increasing it up to 1000,
 and find that TraceFL is both scalable and efficient. Overall,
 we evaluate TraceFL on 20,600 trained client models. These
 experiments exceed prior research’s evaluation complexity and
 fully represent commercial FL usage [34]–[38]. TraceFL is
 implemented in Flower FL [39] and compatible with GPU
 for parallel processing of neuron provenance for compute
intensive models (e.g., GPT).
 TraceFL advances the state of FL debugging with the
 following core contributions:
 • TraceFL localizes the responsible clients for a given
 prediction without modifying the underlying fusion al
gorithm. Moreover, it does not require access to clients’
 training and can solely determine clients’ contributions at
 the central aggregator.
 • TraceFL introduces a unique concept of neuron prove
nance for FL applications to capture the dynamic contri
bution of each client, which helps rank clients based on
 the contribution to a given prediction. TraceFL efficiently
 tracks the contribution of clients in large models like GPT
 containing millions of parameters.
 • TraceFL achieves 99% localization accuracy in localizing
 the responsible client in FL. TraceFL’s localization accu
racy remains high during localizing a faulty client where
 existing baseline [33] achieves 32%.
 • TraceFL is the first approach that is equally effective on
 transformers and CNNs. Even the most sophisticated ML
 debugging approaches work on single model architecture
 and data domains, i.e., either CNNs or transformers.
 • TraceFL significantly advances the field of debugging
 and interpretability in FL, addressing open challenges in
 FL [33], [40] and work with differential privacy and real
world data distributions among FL clients.
 Source Code. TraceFL’s artifact is available at https://github.
 com/SEED-VT/TraceFL.
 II. BACKGROUND AND MOTIVATION
 A. Federated Learning
 Federated Learning enables multiple clients (e.g., mobile
 devices, organizations) to train a shared model without sharing
 their data. This allows the model to be trained using distributed
 data, which can be useful in cases where data is distributed
 across multiple devices or organizations and cannot be easily
 collected and centralized. One algorithm of FL is Federated
 Averaging (FedAvg) [1], which uses the following equation to
 update the global model at each round of the training process:
 K
 Wt+1
 global =
 k=1
 nk
 n W(t)
 k
 (1)
 2
H9
 Accepted at 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)
 Training Data Distribution Across 
Hospitals
 H8
 H7
 Hospital 
H6
 H5
 H4
 H3
 H2
 H1
 H0
 0%
 50%
 100%
 Training Data Points
 Adipose Background
 .
 .
 .
 .
 Hospitals’ Local 
Models
 Debris
 Mucus
 Normal Colon Mucosa
 Lymphocytes
 Smooth Muscle
 Cancer-associated Stroma
 FL Training Round 
Test inputs 
Cancer Mucus
 Global Model
 Mucus
 Cancer-associated 
Stroma
 Classification
 0.34
 H2
 0.33
 0.25
 0.02
 0.02
 H4
 H3
 H1
 H7
 Hospital H2 have data 
points with label 
Mucus
 0.37
 H6
 0.32
 0.01
 0.01
 0
 H5
 H6
 H8
 0.25
 0.02
 0.01
 0.01
 0.01
 H5
 H7
 H3
 H1
 H8
 H2
 0
 H4
 Hospital H6 have 
data points with 
label Cancer
associate Stroma
 Responsible Client Localization with TraceFL
 Fig. 1: Illustration of training, testing, and localization phases of the real-world motivating example. The FL global model
 correctly classifies two colon pathology images (original labels ‘Cacner-associated Stroma’ and ‘Mucus’). During responsible
 client localization, TraceFL accurately identifies the client most responsible for the prediction, i.e., clients trained on data points
 with labels Mucus (Hospital H2) and ‘Cancer-associated Stroma’ (Hospital H6).
 where W(t)
 k and nk represent received weights and size of
 training data of client k in each round t, respectively. The
 variable n represents the total number of data points from all
 clients, and it is calculated as n = K
 k=1 nk. The equation
 states that the global model Wt+1 at the next round is the
 average of the local models from all participating clients at the
 current round. In each round, the clients first train their local
 models using their own data, then send the parameters (e.g.,
 W(t)
 k , nk) to the central server. The central server averages
 the model parameters to produce a global model, which is
 then sent back to the participating devices. This process is
 repeated for multiple rounds (e.g., t from 1 to 100), with each
 client updating its local model using the global model from
 the previous round. The final global model is the result of the
 federated averaging process.
 FL has variations such as Vertical FL [41] and Personalized
 FL [42]. TraceFL primarily focuses on horizontal FL [1],
 similar to previous work on fault localization in FL [10], [33].
 Atypical FL setup involves a few to thousands of clients, such
 as mobile devices, healthcare institutions, or enterprises. FL
 clients exhibit diversity in data distribution and computational
 resources. Data is often non-independently and identically
 distributed (Non-IID), with varying sizes across clients, such
 as hospitals specializing in different medical conditions (Fig
ure 1). All participating clients use the same neural network
 architecture, ensuring compatibility during model aggrega
tion. Additionally, clients have heterogeneous hardware and
 network capabilities (e.g., smartphones to powerful servers),
 impacting their participation and training consistency [43].
 B. Motivation
 Suppose a developer deploys an FL system to diagnose
 colon diseases based on colon pathology images, as shown in
 Figure 1. In this FL system, ten hospitals, identified as H0 to
 H9, collaborate to train a global FL model. Each hospital trains
 its local model, which is then aggregated to form a global
 model. The classification stage in Figure 1 shows a scenario
 where the global model makes a correct prediction on new test
 colon pathology images (e.g., ‘Cancer-associated Stroma’ and
 ‘Mucus’). Since these are correct predictions, the FL developer
 aims to determine which hospital is most responsible for these
 correct predictions so that it can be encouraged to participate
 in future rounds. Since the training data is protected under the
 privacy of medical records and inaccessible to the developer, it
 is challenging to identify the responsible hospital by inspecting
 the raw model weights shared by the hospitals.
 To address this issue, the developer decides to use TraceFL
 to identify the most responsible client behind the correct
 predictions. When enabled during the global model’s pre
diction, TraceFL localizes the hospital with H2 as the one
 responsible for the prediction of ‘Mucus’, and the hospital
 H6 as the one responsible for the prediction of ‘Cancer
associated Stroma’. More specifically, as shown on the right
 in Figure 1, TraceFL ranks hospitals (i.e., clients) based on
 their contributions to each prediction. The score associated
 with each hospital quantifies how responsible a hospital is
 for that prediction. The training data distribution on the left
 shows that H2’s training data include data points labeled ‘Mu
cus’, whereas H6’s training data include data points labeled
 ‘Cancer-associated Stroma’.
 Conversely, hospital H1’s contribution is 0.02 and 0.01
 in the two predictions because it does not have any data
 points with the labels ‘Mucus’ or ‘Cancer-associated Stroma’,
 respectively. Detailed evaluations on two real-world medical
 imaging datasets are presented in Section V-A. Localizing the
 clients responsible for the prediction with TraceFL serves as
 a basis for designing advanced incentivization approaches.
 III. CHALLENGES IN DEBUGGING FL
 Federated Learning poses several challenges in designing
 a debugging technique that reasons about a global model’s
 prediction on an input. Unlike traditional ML training, where
 training data can be easily analyzed, the FL global model
 (Wt+1
 global) is not directly trained on the data. Instead, the global
 3
Acceptedat2025IEEE/ACM47thInternationalConferenceonSoftwareEngineering(ICSE)
 model isgeneratedbyfusingclients’models togetheracross
 manyroundsusingpopularfusionalgorithms.Withnoinsight
 into the training, it is challenging to identifyhowdifferent
 clients influencetheglobalmodel’sbehavior.
 State-of-the-art neural networks, such as Transformers
 (BERT,GPT) andCNNs (ResNet,DenseNet), havevarying
 structures, activation functions, andnumbers of parameters.
 Forexample,GPT[29] isa37-layer (12block)Transformer
 architecture withGELU [23] activation function and 117
 millionparameters.DenseNet [27]has121 layers, 8million
 parameters, anduses theReLU[22]activationfunction.The
 fusion algorithmor the global model does not inherently
 provide any information about individual clients’ contribu
tions. Thus, trackinga client’s contributionamongmillions
 ofparameters isasignificantlychallengingtask.
 Moreover, thedatadistributionacrossFLtraining rounds
 isnon-identical; clients rarelyhave thesamedatapoint.Not
 all clientsparticipate inevery round, andsomeclientsmay
 contain only a fewdata points to train their local model.
 The class label distribution alsovaries across clients. Such
 variabilitycausesmorehurdlesinpreciselyreasoningabouta
 globalmodel’sbehavior.Simplytrackingthestaticweightsof
 theglobalmodelisinadequate,asdifferentsetsofneuronsare
 activatedondifferent inputs, andnot all neuronshaveequal
 importance. Inspecting individual clients’models does not
 helpunderstandtheclient’scontributiontotheglobalmodel
 prediction, as itwill not capture thecumulativebehavior of
 theglobalmodel.
 IV. DESIGNOFTRACEFL
 TraceFL addresses the aforementioned challenges using
 neuron provenance. At a high level, TraceFLdynamically
 tracks the lineage of the globalmodel at the neuron level
 and identifies themost influential clients against a given
 predictionbytheglobalmodel(Wt+1
 global)onaninput.Enabling
 provenance at the neuron level solves the complexities of
 different neural networks architectural design (e.g., number
 of layers andparameters, different activation functions) and
 enablesTraceFLtoworkincrossdomainssuchasimageand
 textclassificationtasks.
 Inessence,TraceFLfirst identifies the influential neurons
 byjointlyanalyzingneuronactivations, thelayerstheneurons
 arein,andgradientswithrespect toindividualneuronsinthe
 globalmodel for agiven test input.Next,TraceFLprecisely
 quantifies the individual contributionof eachcorresponding
 neuron fromeveryparticipatingclient to theneurons in the
 globalmodel. Finally, TraceFLcomputes the total contribu
tionof eachclient to theglobalmodel. These steps collec
tivelyconstructacomprehensiveend-to-endprovenancegraph,
 whichisusedtodebugthecontributionsofclientsinthegiven
 predictionoftheglobalmodel.Algorithm1outlinesthedesign
 ofTraceFL.
 A. DeterminingInfluentialNeurons
 TraceFLfirst aims to identify neurons that actively par
ticipate inanFLglobalmodel’sprediction.Traditional data
 Algorithm1:TraceFL’sApproach
 Input:Letclientsbethelistofclients’modelsparticipatinginthe
 FLtraininground.
 Input:Letglobalmodelbetheaggregatedglobalmodelof
 clientsmodelsafter theendofatraininground
 Input:Let test inputbeaninput
 Output:client2normcontributioncontains thecontributionof
 eachclient inthepredictionoftest input
 1 activatedneurons=[ ];
 //SectionIV-A(Equation2)
 2 y=globalmodel(test input);
 3 foreachneuroninglobalmodeldo
 4 activatedneurons.append(neuron);
 5 neuron2grad=y.backward();
 //SectionIV-B
 6 neuron2prov={};
 7 forneuroninactivatedneuronsdo
 8 neuron2prov[neuron]={};
 9 forclient inclientsdo
 10 cont=0;
 11 forfeatureinneuron.input featuresdo
 12 cont+=client.weight(neuron,feature)×feature.value;
 13 neuron2prov[neuron][client]=cont×neuron2grad[neuron];
 //SectionIV-C
 14 client2contribution={};
 15 forclient inclientsdo
 //Equation6
 16 contribution=0;
 17 forneuroninneuron2provdo
 18 contribution+=neuron2prov[neuron][client];
 19 client2contribution[client]=contribution;
 //Equation7
 20 client2normcontribution=Softmax(client2contribution.values);
 21 returnclient2normcontribution;
 provenanceapproachesmusttracetheparticipationofallinput
 data records in the operation for completeness, eventually
 mappingthemtoindividualoutputsoftheoperation.However,
 trackingtheprovenanceofallneuronswithequal importance
 is wasteful because not all neurons participate equally in
 a model’s prediction. Therefore, tracking the behavior of
 neurons with the same importance in amodel may lead
 to over-approximation (i.e.,more than expected clients are
 classifiedascontributors)whenprovenanceisusedtoidentify
 thecontributingclients.
 The behavior of a neural network on a given input is
 determinedbythesetofactivatedneuronsinthenetwork,and
 different setsofneuronsareactivatedondifferent inputs.We
 leverage this insight andapplyTraceFL’sneuronprovenance
 todynamicallyquantifytheinfluenceofglobalmodelneurons
 oneachpredictionfor thegiveninput.Thisreduces thelike
lihoodofover-approximationbyminimizingthecontribution
 ofneurons thatmaydistort theoutcomewhenthe lineageof
 aspecificneuronisusedtolocalizetheinfluentialclient.
 Mathematically, theoutput of aneuron is z=σ(w·z),
 wherewisthesetofweightsoftheneuron,zistheinput toa
 neuron,andσistheactivationfunction.Oneofthecommonly
 usedactivationfunctions(σ) isReLU [22].Theoutputofσ
 iscalledtheactivationoroutputoftheneuron.Aneuronwith
 ReLUfunction is consideredactive if z>0.Note that the
 4
Accepted at 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)
 output of a neuron (z) is part of the input to the neurons
 of the next layer. Next, TraceFL computes the activation of
 each neuron in the network. Suppose that nj represents the
 j-th neuron in a neural network and the set of all the outputs
 (i.e., activations) of all the neurons in a neural network can be
 represented as {zn1
 ,zn2
 ,...,znj
 }, which captures the complete
 dynamic behavior of the network on a given test input x. Note
 that for the first layer, the input z to neurons will be the input
 x to the model i.e., z = x for the first layer of the neural
 network.
 After computing the global model neurons activations,
 TraceFL’s goal is to find their measurable contribution towards
 the global model’s prediction (y) on an input. In the output
 (y) of the global model, not all the neurons carry equal
 importance. For instance, neurons in the last layers learn better
 and more rigorous features than neurons in the initial layers
 of the network [24]. Since TraceFL aims to localize the client
 that contributed the most towards a prediction, assigning equal
 importance to all neurons will again cause over-approximation
 or even wrong client localization. To enable precise and
 accurate provenance, we must measure the individual influence
 of a neuron on the final prediction.
 TraceFL quantifies the impact of the output of a neuron on
 the global model’s prediction by computing the gradient w.r.t.
 every activated neuron on a given input to Wt+1
 global. Similar to
 taint analysis in program analysis, gradients are sophisticated
 taints that encapsulate the impact of a neuron output on the
 output (y) of the global model. The intuition behind this is that
 the neurons with a higher gradient will likely cause a bigger
 change in prediction. Thus, such neurons are likely to be more
 influential to a model prediction. We use the aforementioned
 insight to find the influence of a neuron in the prediction (y) of
 the global model. The influence, denoted by cnj
 , of a neuron
 nj in the output (y) is the partial derivative of y with respect to
 znj
 , which measures how much y changes when znj 
changes
 slightly. Mathematically, we write it as:
 cnj 
= ∂y
 ∂znj
 (2)
 TraceFL computes the gradients using the automatic dif
ferentiation engine of PyTorch [44]. TraceFL starts from the
 output layer and goes back to the input layer, using the chain
 rule of differentiation at each step. By the end of this phase,
 TraceFL determines the gradient (influence) of global model
 neurons on its output (y). For instance, in the presence of a
 disease in a medical imaging input (e.g., predicting colorectal
 cancer (CRC) from histological slides of tumor tissue), the
 fused neurons of the global model that have learned the
 representation of that particular disease during FL training will
 significantly influence the model’s output (y). These gradients
 are essential in mapping neurons of clients’ models to the most
 influential ones in the global model.
 B. Neuron Provenance Across Fusion
 In this step, TraceFL accurately determines the individual
 contribution of each corresponding neuron from every partic
ipating client to the neurons of the global model. In essence,
 TraceFL maps the outputs of the global model neurons to
 clients’ neurons during prediction. Finding such a mapping
 and its magnitude has two challenges. First, FL uses fusion
 algorithms to merge clients’ neurons statically. Instrumenting
 the fusion algorithms to trace the flow of weights across fusion
 is prohibitively expensive, as numerous clients participate in
 a round where each model may have millions of neurons.
 Second, the influence of clients’ neurons on the neurons of the
 global model (Wt+1
 global) is directly impacted by the output of
 the preceding layer in the global model, i.e., the output of the
 neuron in the global model’s previous layer is the combined
 output of the corresponding neurons of each client in that
 layer. Consequently, attempting to determine clients’ neurons’
 contributions by feeding input to the clients’ model in isolation
 will lead to incorrect neuron provenance, as it cannot capture
 the overall impact of other clients.
 TraceFL leverages the insight that the set of weights of
 a single neuron in the global model is determined by the
 corresponding weights of the neurons in the clients’ models.
 Mathematically, the weights of a single neuron in the global
 model, represented as wg = [w1
 g,w2
 g,··· ,wi
 g], are given by
 the following equation:
 K
 wi
 g =
 k=1
 pk ∗wi
 k
 =p1 ∗wi
 1 +p2 ∗wi
 2 +···+pk ∗wi
 k
 (3)
 Here, wi
 k is the i-th weight of the neuron in the k-th client
 model. The variable pk is nk/n, where nk represents the size
 of training data of client k, and n represents the total number
 of data points from all clients, and it is calculated as n =
 K
 k=1 nk (Equation 1). Given an input z to the neuron wg of
 Wt+1
 global, a client’s contribution can be calculated as follows:
 zout = wg ∗z
 (4)
 =[w1
 g,w2
 g,··· ,wi
 g] ∗ [z1,z2,··· ,zi]
 =w1
 g ∗z1 +w2
 g ∗z2 +···+wi
 g ∗zi
 =[p1 ∗w1
 1 +p2 ∗w1
 2 +···+pk ∗w1
 k]∗z1
 +[p1 ∗w2
 1 +p2 ∗w2
 2 +···+pk ∗w2
 k]∗z2
 +···
 +[p1 ∗wi
 k +p2 ∗wi
 2 +···+pk ∗wi
 k]∗zi
 Here, zi is the i-th input feature to the neuron and zout is
 the output of the neuron. Thus, the contribution of a client k,
 denoted by [tk], in a neuron nj of the global model (Wt+1
 global)
 is given by the following equation:
 [tk]nj 
= (pk ∗ w1
 k ∗ z1 +pk ∗w2
 k ∗z2 +···
 +pk ∗wi
 k ∗zi)∗cnj
 =(pk ∗[w1
 k ∗z1 +w2
 k ∗z2 +···+wi
 k ∗zi])∗cnj
 =cnj 
∗pk ∗
 wi
 k ∗ zi
 i=1
 (5)
 In the above equation, pk ∗ i=1wi
 k ∗ zi is the exact con
tribution of a client k in a neuron nj of the global model.
 5
Accepted at 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)
 The global gradient of neuron nj is cnj 
which is multiplied
 with client contribution to find its actual contribution (i.e.,
 influence) towards the prediction of the global model. For
 instance, if the contribution of a client k is high in a neuron
 nj but globally the neuron nj has minimal influence on
 the global model prediction then cnj 
will scale down the
 contribution of the client in the given neuron nj. Note that zi
 represents the i-th output of the previous layer in the global
 model during prediction. At the end of this stage, TraceFL
 constructs a neuron provenance graph that traces a global
 model’s prediction to influential neurons in the global model
 (Wt+1
 global), which are further traced back to individual neurons
 in the clients’ models.
 C. Measuring Client’s Contribution
 To find the end-to-end contribution, we must accumulate
 neuron-level provenance, cnj 
∗ pk ∗ i=1 wi
 k ∗ zi, of a given
 client’s model to derive its complete contributions toward the
 global model’s prediction. A client’s overall contribution to
 the global model prediction is determined by the sum of
 the client’s contribution to the neurons of the global model.
 Specifically, if the set of neurons of the global model is
 denoted by n1,n2,··· ,nj, then the total contribution (Tk) of
 the client k can be calculated using Equation 5 as follows:
 Tk =βn1 
∗[tk]n1 
+βn2 
∗[tk]n2 
+··· +βnj 
∗[tk]nj
 =([cn1 
∗
 wi
 k n1 
∗ zi
 n1
 ]n1 
+ [cn2 
∗ 
i=1
 zi
 n2
 ]n2 
+ ··· + [cnj 
∗ 
wi
 k n2
 ∗
 i=1
 wi
 k nj 
∗ zi
 nj
 ]nj
 ) ∗ pk
 i=1
 (6)
 β is an importance factor that TraceFL computes using
 an exponential decay method for each neuron based on its
 position in the neural network. Specifically, TraceFL assigns
 higher importance to the last layers and lower importance
 to the earlier layers to minimize the noisy contributions,
 based on the evidence presented elsewhere [24]. [tk]nj 
is the
 contribution of the client k in neuron nj, zi
 nj 
is the i-th input
 feature to neuron nj, and wi
 k nj 
is the i-th weight of neuron
 nj in the client k model. Using Equation 6 we can compute,
 for each client k, the total contribution towards the global
 model prediction. Thus, the client with max contribution is
 the client that has the most influence on the global model
 prediction. To make the client contribution more interpretable,
 we normalize the client contribution by using the softmax
 function as follows:
 ˜
 Tk = eTk
 K
 i=1 eTi
 ˜
 (7)
 Tk is the normalized contribution of the k-th client, which
 is now a probability value between 0 and 1, representing the
 relative influence of client-k on the global model output y for
 a given input.
 TraceFL concludes its neuron provenance capturing tech
nique by listing the total contribution of each participating
 client in an FL round towards a global model’s prediction
 on a given input. The magnitude of the contributions can be
 interpreted as a confidence level of TraceFL in identifying the
 source of the global model’s prediction. Given that the total
 confidence scores of all clients cannot exceed 1, if a client has
 a contribution score of 0.6, it implies that no other client can
 surpass a score of 0.4. This makes the client most influential
 in determining the global model prediction and most likely
 responsible for the prediction.
 Enable TraceFL to Use GPU. By design, TraceFL is com
patible with hardware accelerators and can fully harness their
 parallelizability. The primary dependency of TraceFL is cap
turing the output of previous layer neurons in the global model
 for input to the next layer neurons, which inherently exists in
 inference as well. Additionally, TraceFL computes gradients
 using Equation 2, leveraging the chain rule of differentiation
 that the hardware accelerators can parallelize. Next, Equation 5
 dissects the global model neuron and computes the contribu
tion based on the previous layer neurons’ outputs (z) of the
 global model, which is the cumulative output of all clients’
 neurons in that previous layer. This is the only dependency in
 TraceFL. Once TraceFL has the cumulative output from the
 previous layer neurons, it parallelizes the process to find the
 contribution of a client in each neuron of the global model in
 the current neuron layer and ultimately the total contribution
 using Equations 6 and 7. These optimizations in TraceFL
 enable neuron-level provenance for neural networks primarily
 deployed on GPUs, such as GPT.
 V. EXPERIMENTAL EVALUATIONS
 We design experiments to evaluate TraceFL’s accuracy in
 localizing the client responsible for a global model’s prediction
 on an input. We ask the following research questions.
 • How accurate is TraceFL in identifying the client(s)
 responsible for a global model’s prediction?
 • Is TraceFL equally accurate on FL of different models
 and architectures such as CNNs and transformers (GPT)?
 • Howaccurate TraceFL is in localizing clients responsible
 for mispredictions by global model?
 • Does TraceFL remain effective with varying data distri
butions and differential privacy?
 • Can TraceFL scale to a large number of clients?
 • What is the runtime performance of TraceFL?
 Models and Datasets. We evaluate TraceFL on state-of-the-art
 and commercially used CNNs, including ResNet-18 [26] and
 DenseNet-121 [27], as well as the two most popular trans
former models, BERT [28] and GPT [29] to demonstrate the
 wide applicability of TraceFL. We train ResNet and DenseNet
 on CIFAR-10 [45] and MNIST [46]. These network-dataset
 combinations are widely used and serve as standardized
 benchmarks in practice [1], [37]. We also evaluate TraceFL
 on real-world medical imaging datasets, including the Colon
 Pathology dataset [30] and Abdominal CT dataset [31], [32],
 to demonstrate its usability in complex real-world FL systems.
 The Colon Pathology dataset contains 107,180 biomedical
 images representing nine classes of colon pathology, while the
 Abdominal CT dataset contains 58,830 images of abdominal
 CT scans representing 11 classes. More details about these
 6
Acceptedat2025IEEE/ACM47thInternationalConferenceonSoftwareEngineering(ICSE)
 datasets can be found in [47], [48]. For NLP tasks, we
 evaluateTraceFLonBERTandGPTmodels trainedon the
 DBpedia andYahooAnswers datasets [49]. TheDBpedia
 dataset contains560,000trainingsamplesand70,000testing
 samples,whiletheYahooAnswersdatasetcontains1,400,000
 trainingsamplesand60,000testingsamples, representing14
 and10classes, respectively.
 DataDistributionAmongClients.WeuseDirichletdistribu
tion inFL todistributenon-overlappingdatapoints among
 clientsineachround.ThisisthestandardFLdatadistribution
 methodproven toproduce real-worlddistribution[37], [38],
 [50], [51]. The parameter (α) inDirichlet ranges from[0,
 ∞), determining the level ofNon-IID in experiments. For
 instance,whenαequals100, it replicatesuniformlocaldata
 distributions,whilesmallerαvalues increase theprobability
 thatclientspossesssamplesfromasingleclass[51].Avalue
 of 0.5 is a commonpractice inpriorwork [37], [50].We
 use an even stricter parameter value of 0.3 to stress test
 TraceFLand demonstrate its usability inmore challenging
 cases.Nevertheless,SectionV-C1performssensitivityanalysis
 byvaryingαfrom0.1to1.Thesesettingsinherentlysimulate
 varyingdegreesof labeloverlapamongclients.Toexplicitly
 manageoverlappinglabels,pathologicaldatadistributionscan
 beemployed[37],asshowninFigure1.Thepathologicaldata
 distribution is available inTraceFL’s artifact. Furthermore,
 TraceFL’s artifact contains configurable data distributions
 amongclientsandallowsevaluationsonvaryingnumbersof
 test inputs.
 ExperimentalEnvironment.To resemble real-worldFL,we
 deploy our experiments inFlower FL [39], running on an
 enterprise-levelclusterofsixNVIDIADGXA100[52]nodes.
 Eachnodeisequippedwith2048GBofmemory,at least128
 cores, andanA100GPUwith80GBofmemory.
 We vary training rounds between 15 to 80with clients
 ranging from100 to 1000, thus testingTraceFLonmore
 configurationsthananyrelatedwork[51], [53].Tenrandomly
 selected clients participate in each round, reflecting a real
worldscenariowherenotalltheclientsparticipateinthegiven
 round[54].WeevaluateTraceFLwithFedAvg[1].
 Localization Accuracy. To measure the performance of
 TraceFL,weevaluate theaccuracyofTraceFLinfindingthe
 responsibleclients.Forbrevity,werefertothisaslocalization
 accuracy,whichisdefinedasfollows:Giventheznumberof
 testinputstotheglobalmodel(Wt+1
 global),ifTraceFLaccurately
 locatesmtimes theclients responsiblefor thezpredictions,
 thenthelocalizationaccuracyism∗100
 z .
 A. TraceFL’sLocalizationAccuracyinCorrectPredictions
 Identifying the clientsmost responsible for correct pre
diction is a key debugging objective that helps encourage
 future participation of those clients to improve the overall
 FLaccuracy. Note that TraceFLdirectlydoes not improve
 theFLmodelaccuracy. Instead, it reasonsabout thebehavior
 of theFLglobalmodelwhichanFLdeveloper canuse to
 improvetheFLmodelaccuracy(e.g., selectingclientswhich
 arecontributingmoreintheFLglobalmodelpredictions).
 0 10 20
 25
 50
 75
 100
 a) Colon-Pathology
 ResNet
 0 10 20
 b) Abdominal-CT
 ResNet
 0 10 20
 c) Colon-Pathology
 DenseNet
 0 10 20
 d) Abdominal-CT
 DenseNet
 0 10 20
 25
 50
 75
 100
 e) DBpedia
 GPT
 0 10 20
 f) Yahoo-Answers
 GPT
 0 10 20
 g) DBpedia
 BERT
 0 10 20
 h) Yahoo-Answers
 BERT
 0 20 40
 25
 50
 75
 100
 i) MNIST
 ResNet
 0 20 40
 j) CIFAR10
 ResNet
 0 20 40
 k) MNIST
 DenseNet
 0 20 40
 l) CIFAR10
 DenseNet
 Communication Rounds
 Accuracy (%)
 TraceFL Localization Accuracy FL Model Accuracy
 Fig.2:TraceFLperformanceonmultipledatasetsandmodels
 bothontextandimageclassificationtasks.
 TraceFL’sneuronprovenance traces predictions back to
 clientstrainedonthoselabels, rankingclientsbytheircontri
bution.TraceFLreturnsarankedlistofclients indescending
 orderof responsibilitytowardsaprediction,where theclient
 withthehighest scoreis likelytobemost responsible.
 WeevaluateTraceFL’s localizationaccuracyon two real
world medical imaging datasets, two standardized image
 datasets, and twoNLPclassificationdatasets usingResNet,
 DenseNet,BERT, andGPTmodels resulting inover 12FL
 configurations spanning a total 400 FL rounds and 4000
 models.Weverifyif themost responsibleclient returnedby
 TraceFLcontains thedatawith the label thatwas correctly
 predicted by the global model.Wemeasure the accuracy
 on at least 10 test inputs in each round. Figure 2 shows
 TraceFL’sperformance in localizingresponsibleclients.The
 X-axis represents training rounds, while theY-axis shows
 theFLglobalmodel’sclassificationaccuracyandTraceFL’s
 localizationaccuracy.
 WeincludetheFLglobalmodel’saccuracytodemonstrate
 the trainingprogression.Higher globalmodel accuracy im
provesneuronprovenanceconfidence,aidingTraceFL’seffec
tiveness.Globalmodelaccuracyhelpscalibratetheprovenance
 resultsbecauselowermodelaccuracyleadstolowconfidence
 in prediction, which transitively reduces the confidence of
 neuronprovenance,causingadditionalchallengesforTraceFL.
 As trainingprogresses andmore clientswithunique labels
 participate, theglobalmodel’saccuracyimproves.
 Our results indicate that TraceFL consistently localizes
 responsible clients regardless of the globalmodel’s perfor
mance,neuralnetworkarchitecture,numberoftrainingrounds,
 or dataset. It accurately identifies contributions even from
 clients participating for the first time. Across different FL
 7
Accepted at 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)
 Domain Dataset
 Dirichlet
 Distribu
tion (α)
 FedDebug
 Accuracy
 (%)
 TraceFL
 Accuracy
 (%)
 0.3 0.00 100
 Abdominal-CT 0.7 21.5 100
 1.0 44.4 100
 0.3 0.00 100
 Image
 Colon-Pathology 0.7 54.7 100
 1.0 68.7 100
 0.3 20.0 100
 CIFAR10 0.7 11.3 100
 1.0 22.0 100
 0.3 14.0 100
 MNIST 0.7 86.0 100
 1.0 36.0 100
 0.3 NA 96.7
 Text
 DBpedia 0.7 NA 94.0
 1.0 NA 97.3
 0.3 NA 100
 Yahoo-Answers 0.7 NA 100
 1.0 NA 100
 TABLE I: Comparison of TraceFL with FedDebug on lo
calizing clients responsible for misprediction. FedDebug is
 compatible with image classification only and is effective
 under specific data distribution (i.e., α = 1).
 settings, TraceFL’s average localization accuracy on image
 classification tasks is 98.96%, and in text classification tasks,
 it is 99.59%, demonstrating its broader effectiveness and
 applicability to domains other than image classification.
 Takeaway. On average, TraceFL achieves localization accuracy
 of 99.12% across all FL experiments settings.
 B. TraceFL’s Localization Accuracy in Mispredictions
 FL’s global model can exhibit unwanted behavior (e.g.,
 mispredictions) due to intentional or unintentional faults in
 the training data of clients. Mislabelling in training data may
 occur due to faulty sensors, human error in labeling data, or,
 in some cases, adversarial attacks [1]–[5]. Finding a client
 responsible for such behavior is a crucial debugging goal that
 helps FL developers exclude such clients from participating in
 future rounds to improve the global model’s quality.
 To evaluate TraceFL’s localization accuracy on mispredicted
 labels by a global model, we design the following experiments
 with ten clients. Similarly to prior work on fault localization
 in FL, FedDebug [33], we select one client in an FL round
 and flip a specific label in its training data to make it faulty.
 The inclusion of such clients influences the global model to
 make mispredictions. For instance, in the medical dataset, we
 f
 lip the label ‘Cancer-associated Stroma’ to ‘Adipose’ in the
 Colon Pathology dataset to reflect a faulty hospital containing
 incorrect label data that may occur due to misdiagnosis.
 Table I shows the results. TraceFL outperforms FedDebug
 significantly and can operate in cross-domain tasks of image
 and text classification without any change in its approach.
 This is expected since FedDebug, by construction, applies to a
 different problem setting, i.e., debugging the model instead of
 the prediction, and it primarily targets a specific set of Non-IID
 data distributions. Even on image classification tasks, TraceFL
 outperforms FedDebug in terms of localization accuracy. For
 a) Colon-Pathologyb) Abdominal-CT
 100
 50
 c) MNIST
 0.5
 d) CIFAR10
 100
 50
 Accuracy (%)
 1.0
 0.5
 1.0
 e) Yahoo-Answers
 0.5
 f) DBpedia
 1.0
 0.5 
1.0 
0.5
 Avg. TraceFL Localization Accuracy
 1.0 
0.5 
Max FL Model Accuracy
 1.0
 Fig. 3: TraceFL performance on different data distributions.
 The X-axis represents the values of Dirichlet alpha.
 instance, in Abdominal-CT with α = 1, FedDebug’s average
 accuracy is 44.4% while TraceFL’s accuracy is 100%.
 Takeaway. TraceFL achieves 99.3% average localization ac
curacy across 18 FL settings, whereas FedDebug’s average
 localization accuracy is only 32% on image classification.
 C. TraceFL’s Robustness
 Varying the client’s data distribution and applying differ
ential privacy (DP) techniques in FL pose additional hurdles
 to FL in achieving high model accuracy, which, in turn, may
 pose challenges to TraceFL in keeping its high localization
 accuracy. Therefore, to add rigor to our experiments, we
 evaluate the impact of these two additional FL settings on
 TraceFL localization accuracy. In this section, we only include
 results from the most challenging experiment setting due to
 space constraints.
 1) Varying Data Distribution: Different distributions of
 data among clients can impact the FL training process. For
 instance, in a highly challenging data distribution (α = 0.1),
 FL training suffers from low global model accuracy. This
 is a known phenomenon in FL [37], where the FL fusion
 algorithm struggles to aggregate clients’ models trained on
 severely heterogeneous training data. To mitigate bias towards
 a specific Dirichlet data distribution, we evaluate TraceFL on
 varying the value of α from 0.1 to 1.0, showing the impact of
 different data distributions on TraceFL’s localization accuracy.
 Figure 3 shows the results of this experiment on all six
 datasets. The X-axis represents the value of α in the Dirichlet
 distribution, while the Y-axis represents the accuracy. For a
 value of α, we report the maximum accuracy achieved by
 the global model across all the rounds as FL model accuracy
 and the average accuracy of TraceFL across all the rounds as
 localization accuracy of TraceFL.
 As expected, the FL training accuracy decreases as the
 value of α decreases. This is because the clients have varying
 data both in terms of quantity and labels. For instance, when
 α = 0.1 in Figure 3-(a), the maximum FL global model
 accuracy observed across all rounds is 35.25% and when
 α = 0.5 the maximum accuracy is 83.4%. Since GPT is
 an advanced neural network architecture that learns better in
 comparison to DenseNet, the FL training accuracy is higher in
 GPT on lower α values as well. Overall, TraceFL localization
 8
Acceptedat2025IEEE/ACM47thInternationalConferenceonSoftwareEngineering(ICSE)
 DPNoise DPSensitivity FLModelAc
curacy%
 TraceFLAvg.
 Accuracy%
 0.003 15 97.36 100
 0.006 10 97.90 100
 0.012 15 88.81 100
 TABLEII:ResultsofTraceFLwithDPinFL.
 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030
 Differential Privacy Noise
 0
 50
 100
 Accuracy (%)
 Avg. TraceFL Localization Accuracy FL Model Accuracy
 Fig.4: ImpactofDPnoiseonFLtrainingaccuracy.
 accuracy is99.76%, onaverage, acrossall valuesofα.The
 lineplotsshownosignificantchangeinTraceFLlocalization
 accuracy, demonstratingTraceFL’s robustness inchallenging
 real-worlddatadistributions.
 2)Differential Privacy-EnabledFL: Differential privacy
 (DP) isaprivacy-preservingmechanismthatensures that the
 output of amodel does not reveal any information about
 the individual datapoints.DP inFL[55] addsnoise to the
 weights of amodel toprotect against anadversary stealing
 or recovering the individual training data points. However,
 adelicatebalance isneeded inDPbetween thenoise tobe
 addedandmodelaccuracy,asaddingtoomuchnoiseseverely
 decreases themodel’saccuracy.
 WeevaluateTraceFL’s robustnesswhenDPisenabled in
 FL, using standardDP settings inFL that provide optimal
 privacyandmodelaccuracy,asmentionedinpriorwork[55].
 TableIIpresents theresultsof thisexperiment, andFigure4
 shows the impact of noiseon theFL trainingaccuracy.As
 expected, theFLmodel’s accuracydecreaseswhen theDP
 noiseincreasesandviceversa.
 However,TraceFLmaintainsitsperformanceinDP-enabled
 FL.AsDPaddsnoisetothemodelweights,theglobalmodel’s
 output is still basedon itsneurons’ activationson thegiven
 input.Thus,TraceFL’sworkingprincipleremains intact, and
 it successfully traces back to the source of the prediction
 basedon theglobalmodel’sneuronprovenance.Wewant
 to emphasize that TraceFLdoes not recover the individual
 clients’datapoints. Itonlyidentifiestheresponsibleclientsin
 rankedorder.Overall,wefindthatTraceFLis robust against
 the use of differential privacy inFLwhere it achieves an
 average localizationaccuracyof 99%inGPTandDBpedia
 dataset (Figure4andTableII).
 Takeaway. TraceFL is robust tochallenging real-worlddata
 distributions and the use of differential privacy, achieving
 approximately99%localizationaccuracy.
 D. TraceFL’sScalability
 Weassess thescalabilityofTraceFLacross threedifferent
 dimensions: (1)byincreasethetotalclients, (2)byincreasing
 TotalClients FLModelAccuracy
 %
 TraceFLAvg.Accu
racy%
 200 98.49 99.76
 400 98.29 99.76
 600 98.39 100
 800 98.10 100
 1000 98.05 99.52
 TABLE III: Scalability results of TraceFLwith different
 numberofclientswithGPT.
 0 20 40 60
 0
 50
 100
 TraceFL Localization Accuracy
 FL Model Accuracy
 Communication Rounds
 Accuracy (%)
 Fig.5:TraceFL’sscalabilitywhen#of rounds increase
 theclient participation, and(2)by increasing thenumberof
 rounds. First, we vary the number of clients from200 to
 1000andmeasureifTraceFLcanstillaccuratelylocalizethe
 responsibleclient.Weuse thestate-of-the-artneuralnetwork
 GPTand theDBpediadataset.Table III presents the results
 of the scalability experiment. We observe that TraceFL’s
 performanceremainsconsistent,withanaverage localization
 accuracyof99%across200to1000clientsoveratotalof75
 FLtrainingrounds.Thisexperiment significantlyexceeds the
 scaleofexperimentsperformedbypriorwork[33].
 Whenwe vary the number of participating clients per
 roundfrom20to50,TraceFL’sperformanceremainsstable,
 achieving100%localizationaccuracyacross60FLtraining
 rounds.Priorworkhasshownthatevenatanenterprisescale,
 only a fewclients participate in a single FL round [34],
 [36]. Furthermore, we evaluate the scalability of TraceFL
 over up to 80 rounds with 400 clients in total. Figure 5
 demonstrates thatTraceFLmaintainsconsistent performance
 withanaverage localizationaccuracyof98%.These results
 indicate that TraceFL is scalable and canhandle numerous
 clientsandroundswithoutcompromisingitsperformance.
 Takeaway.Overall,TraceFLiscapableofhandlingtheprove
nanceofmillionsofneurons in theneural network toaccu
ratelyidentifythemostresponsibleclient.InFLsettingsofup
 to80FLtrainingroundsand1000clientsusinglargemodels
 suchasGPTandBERT—and6different datasets, TraceFL
 achievesanaveragelocalizationaccuracyof99.20%.
 E. TraceFL’sLocalizationTime
 Weevaluate theruntimeperformanceofTraceFLbymea
suring the time TraceFL takes to accurately localize the
 responsible clients in FL. As mentioned before, there is
 noexistingmethod that localizes the responsibleclients for
 both correct and incorrect predictions. The closed related
 work toTraceFL is FedDebug,whichonly localizes faulty
 clients. Thus,we compareTraceFL’s localization timewith
 FedDebug’sfaultylocalizationtime.
 9
Avg. Time (s)
 10
 Accepted at 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)
 5
 0
 FedDebug Time (s) TraceFL Time (s)
 Fig. 6: Client localization of TraceFL vs. FedDebug.
 Figure 6 presents the localization times per dataset for both
 TraceFL and FedDebug, averaged across faulty client local
ization settings. Note that FedDebug is not compatible with
 the text classification models; therefore, its localization times
 for the two text datasets are not available. TraceFL takes, on
 average, 3.7 seconds to localize the responsible client, whereas
 FedDebug’s faulty client’s localization time is 1.1 seconds
 on average. This is expected as TraceFL requires computing
 gradients of neuron outputs, whereas FedDebug compares raw
 neuron activations. While TraceFL’s localization time is higher
 than FedDebug, it is almost negligible compared to the FL’s
 per round training time (in minutes if not hours [33]).
 Takeaway. TraceFL compensates for the marginally slower
 localization time with much broader debugging support for
 model architecture, text data domains, and general-purpose
 reasoning in FL.
 F. Threat to Validity and Limitations
 There are two primary threats to the validity of the results.
 First, in our experiments, we select a random subset of
 clients to participate in every round. A different sequence of
 randomly selected participating clients may alter the TraceFL’s
 accuracy. We mitigate this threat by performing responsible
 client localization on every round and then reporting the
 average localization accuracy across rounds. Second, the same
 Dirichlet distribution (α) may provide a different distribution
 of the training data across clients. Even when the value α
 is the same, the localization accuracy of TraceFL may vary
 slightly. We mitigate this threat by averaging the localization
 accuracy across rounds and also measuring the localization
 accuracy on different datasets and models. TraceFL is designed
 for classification tasks in FL and may not be directly applicable
 to non-classification tasks such as text generation [56] and
 embeddings generation [57].
 VI. RELATED WORK
 Debugging and Interpretability in Machine Learning. As the
 complexity of neural network models continues to increase, the
 need for interpretability techniques becomes more crucial and
 important. Interpretability techniques are used to understand
 the inner workings of a neural network. These techniques
 try to explain the decisions made by the model, and how
 the model makes these decisions. This is important for many
 reasons, including the ability to explain which input features
 are important to a model’s output, to understand the model’s
 behavior, and to identify potential biases and errors in a
 trained model. Several approaches, such as Integrated Gradi
ents [58], Gradient SHAP [59], DeepLIFT [60], Saliency [61],
 Guided GradCAM [62], Occlusion (also called sliding window
 method) [63], and LIME [64], exist which evaluate the contri
bution of each input feature to model’s output. For instance,
 Integrated Gradients [58] evaluates the contribution of each
 input feature by calculating the integral of gradients w.r.t.
 input. This is done along the path from a selected baseline to
 the given input. Occlusion involves replacing each contiguous
 rectangular region with a predetermined baseline or reference
 point and measuring the difference in the model’s output. This
 approach is based on perturbations and provides a way to
 evaluate the importance of input features by measuring the
 change in the model’s output.
 Existing debugging techniques [65]–[69] are designed to
 identify issues and enhance the performance of a single neural
 network in centralized ML. These methods typically require
 access to training data, which is prohibited in FL. For example,
 NPC [68] constructs a Decision Graph using training data.
 Furthermore, these approaches have not been evaluated on
 modern neural network architectures such as Transformers.
 Almost all existing debugging and interpretability ap
proaches are inapplicable in FL, as by design, they solve an
 orthogonal problem — identifying the important feature in
 the input responsible for a prediction instead of clients. This
 distinction is critical because the training data or the training
 process is completely inaccessible in FL. Existing approaches
 require access to the client’s data. Furthermore, they are only
 designed for a single neural network, but the FL global model
 is a mixture of clients’ models participating in the given round.
 Operating these techniques on FL would require us to first
 identify a suspicious client’s mode–a problem that TraceFL
 solves. Even if such techniques are applied to a client’s model,
 the resulting feedback is not immediately actionable and
 constructive. TraceFL is designed to address the limitations
 of the existing debugging approaches and added challenges of
 FL, such as distributed training, inaccessibility to clients, and
 the mixture of models.
 Abdominal-CT
 Colon-Pathology
 CIFAR10
 FedDebug [33] introduces differential testing in FL to
 identify faulty clients by capturing each client’s activations
 for a given input and localizing the client(s) whose behavior
 deviates from others. Building on FedDebug, a backdoor
 detection technique in FL is presented in [10]. Additionally,
 FedGT [11] aims to identify malicious clients in FL; however,
 it is limited to scaling up to 15-30 clients and has not been
 tested on advanced architectures like GPT.
 Despite their contributions, these existing methods target a
 narrower problem under a specialized setting. FedDebug is
 limited to image classification tasks using CNNs, restricting
 its applicability to Transformer architectures. Additionally, it is
 designed primarily for faulty clients and IID distributions [37],
 as demonstrated in Table I. In contrast, TraceFL targets
 a broader debugging problem using a domain-agnostic and
 MNIST
 DBpedia
 Yahoo-Answers
 10
Accepted at 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)
 highly accurate client localization mechanism applicable to
 diverse neural network architectures, data types, and distribu
tions through its novel fine-grained neuron provenance.
 There has been recent work on ensuring accountability
 in FL systems. A vast majority of solutions leverage the
 blockchain to ensure accountability [70]–[74]. Some of these
 works (BlockFLow [73], BlockFLA [74]) design an FL system
 that uses the Ethereum blockchain to provide accountability
 and monetary rewards for good client behavior. However,
 all these systems require utilizing the blockchain and entail
 significant modifications to the existing FL system, presenting
 a barrier to adoption. TraceFL, in contrast, can work with any
 existing system without modifications.
 Provenance Approaches in ML. Provenance has been ex
tensively studied for both ML and dataflow programs [18]
[20], [75], [76]. They address various issues such as re
producibility [17], [77]–[80], provide debugging and testing
 granularities [76], explainability [20], and mitigating data poi
soning attacks [14]–[16]. In the context of machine learning,
 provenance tracks the history of datasets, models, and exper
iments. This information is used to select the interpretability
 of neural network predictions and reproducibility. Provenance
based approaches are important to create ML systems that
 generate reproducible results [17], [77]–[80]. For instance,
 Ursprung [17] captures provenance and lineage by integrating
 with the execution environment and records information from
 both system and application sources of an ML pipeline.
 Ursprung does not require changes to the code and only adds
 a small overhead of up to 4%.
 VII. CONCLUSION
 We introduce the concept of neuron provenance and de
veloped a debugging and interpretability tool, TraceFL, for
 FL. TraceFL accurately identifies the primary contributors to
 a global model’s behavior. Our evaluations show that TraceFL
 achieves an impressive average localization accuracy of 99%.
 Furthermore, TraceFL also outperforms the existing fault lo
calization technique. We provide a reusable functional artifact
 of TraceFL in the Flower framework to have an immediate
 practical impact in real-world FL deployment, addressing the
 open challenges of debugging and interpretability in FL.